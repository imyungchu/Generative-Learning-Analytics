# learner_app_compact_0515.py
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# Streamlit app: compact view â€“ moves verbose text output into an expander
# Run with:  streamlit run learner_app_compact_0515.py

import streamlit as st
import pandas as pd
import json
from collections import defaultdict
from clustering_utils_0514 import readable_label, make_full_graph
import streamlit.components.v1 as components

st.set_page_config(layout="wide")
st.title("ğŸ§¬ Learner Error â†’ Replacement â†’ Collocation Concept Groups")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Data helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
@st.cache_data
def load_json(path: str):
    with open(path, encoding="utf-8") as fh:
        return json.load(fh)

@st.cache_data
def load_csv(path: str):
    return pd.read_csv(path)

DATA = load_json("data/merged_output_0515.json")
CSV  = load_csv("data/flat_report_all.csv")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Main UI â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
err = st.text_input("Search for an error word:"  ).strip()

if err:
    # 0. basic validity check
    if err not in DATA or not DATA[err].get("replacements"):
        st.warning("No data found for this error word.")
        st.stop()

    # 1. JSON-level stats
    total_count = sum(r["count"] for r in DATA[err]["replacements"].values())
    st.header(f"ğŸ”¹ Error: '{err}' â€” corrected {total_count} times")

    # 2. CSV slice & ordering
    df = CSV[CSV["éŒ¯èª¤å­—"] == err].copy()
    if df.empty:
        st.info("No detailed CSV data available for this word.")
        st.stop()

    sort_cols = [
        "æ­¤éŒ¯èª¤ç¸½æ¬¡æ¬¡æ•¸",
        "æ­¤éŒ¯èª¤-æ­£ç¢ºç¸½éŒ¯èª¤æ¬¡æ•¸",
        "æ­¤éŒ¯èª¤æ­£ç¢ºç´¯ç©æ­¤æ­é…è©åˆ†é¡å”¯ä¸€æ¬¡æ•¸",
        "æ­¤éŒ¯èª¤-æ­£ç¢ºæ­é…ç¸½éŒ¯èª¤æ¬¡æ•¸",
    ]
    df = df.sort_values(sort_cols, ascending=[False]*4)

    df_with_colloc = df[df["æ­¤æ­é…è©"].notna() & (df["æ­¤æ­é…è©"] != "")]
    df_no_colloc   = df[df["æ­¤æ­é…è©"].isna()  | (df["æ­¤æ­é…è©"] == "")]

    # -----------------------------------------------------------------------
    # 3. EVERYTHING from here down to the graph lives inside an expander âœ”ï¸
    # -----------------------------------------------------------------------
    with st.expander("ğŸ“Š Correction details", expanded=False):
        # 3-a. detailed replacement-collocation summary
        st.subheader("1. Error â†’ Replacement â†’ Collocation")
        summary_data = defaultdict(lambda: {"count": 0, "collocations": {}})

        for rep, grp in df_with_colloc.groupby("æ­£ç¢ºæ­é…"):
            rep_count = grp["æ­¤éŒ¯èª¤-æ­£ç¢ºç¸½éŒ¯èª¤æ¬¡æ•¸"].iloc[0]
            summary_data[rep]["count"] = rep_count

            for concept, sg in grp.groupby("æ­¤æ­é…è©åˆ†é¡"):
                collocs = sg["æ­¤æ­é…è©"].unique().tolist()
                # keep the largest collocate list we meet for each (rep, concept)
                if len(collocs) > len(summary_data[rep]["collocations"].get(concept, [])):
                    summary_data[rep]["collocations"][concept] = collocs

            concept_strings = [
                f"- **{readable_label(c)}**: {', '.join(v)}"
                for c, v in summary_data[rep]["collocations"].items()
            ]
            st.markdown(f"* â†’ **{rep}** ({rep_count}Ã—)  " + "â€‚|â€‚".join(concept_strings))

        if not df_no_colloc.empty:
            st.markdown(
                "##### Replacements with **no** collocations\n"
                + ", ".join(sorted(df_no_colloc["æ­£ç¢ºæ­é…"].unique()))
            )

        # 3-b. concept-centric overview
        st.subheader(f"2. Overall Replacement Groups for '{err}'")
        for concept, subdf in df_with_colloc.groupby("æ­¤æ­é…è©åˆ†é¡"):
            label = readable_label(concept)
            repls   = ", ".join(sorted(subdf["æ­£ç¢ºæ­é…"].unique()))
            collocs = ", ".join(sorted(subdf["æ­¤æ­é…è©"].unique()))
            st.markdown(f"- **{label}**  \n  "
                        f"Replacements: {repls}  \n  "
                        f"Collocations: {collocs}")

        # 3-c. optional raw table
        if st.checkbox("Show raw table â†”", value=False, key="show_table"):
            st.dataframe(df_with_colloc, use_container_width=True)

        # 3-d. â€œtop confusableâ€ quick view
        top_reps = (
            df_with_colloc
            .groupby("æ­£ç¢ºæ­é…")["æ­¤éŒ¯èª¤-æ­£ç¢ºç¸½éŒ¯èª¤æ¬¡æ•¸"]
            .max()
            .sort_values(ascending=False)
        )
        st.markdown(
            "##### Top confusable replacements\n"
            + ", ".join(top_reps.head(10).index)
        )

    # -----------------------------------------------------------------------
    # 4. Build graph-friendly data from `summary_data`
    # -----------------------------------------------------------------------
    graph_data = {
        err: {
            "replacements": {},
            "clusters": {},
        }
    }
    for rep, info in summary_data.items():
        graph_data[err]["replacements"][rep] = {
            "count": info["count"],
            "collocations": {},
        }
        for concept, collocs in info["collocations"].items():
            graph_data[err]["clusters"].setdefault(concept, collocs)
            graph_data[err]["replacements"][rep]["collocations"][concept] = {
                c: 1 for c in collocs
            }

    # 5. Show the PyVis graph (lazy-render on click)
    if st.button("ğŸ—ºï¸ Show interactive graph"):
        st.info("Building graph â€“ please waitâ€¦")
        net, _ = make_full_graph(graph_data, height="750px", width="100%")

        path = "pyvis_graph.html"
        net.write_html(path, notebook=False, open_browser=False)
        with open(path, "r", encoding="utf-8") as fh:
            components.html(fh.read(), height=750, scrolling=True)


with st.expander("ğŸ” Browse whole CSV", expanded=True):
    sort_by = st.multiselect(
        "Sort by columns:",
        options=list(CSV.columns),
        default=["æ­¤éŒ¯èª¤-æ­£ç¢ºç¸½éŒ¯èª¤æ¬¡æ•¸"],
        key="csv_sort",
    )
    st.dataframe(
        CSV.sort_values(sort_by, ascending=False),
        use_container_width=True,
    )
